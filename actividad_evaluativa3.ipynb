{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MarioSolano98/actividad-evaluativa3-AC/blob/main/actividad_evaluativa3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Punto 4"
      ],
      "metadata": {
        "id": "xKhkF47YKElY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Importar Librerias"
      ],
      "metadata": {
        "id": "MHF-78Jkafab"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Clonar el repositorio\n",
        "!git clone https://github.com/MarioSolano98/actividad-evaluativa3-AC.git"
      ],
      "metadata": {
        "id": "8-eJivkXLQ2g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ymgyXd9kHQIf"
      },
      "outputs": [],
      "source": [
        "# Importar librerías esenciales\n",
        "import pandas as pd            # Para manipulación y análisis de datos\n",
        "import numpy as np             # Para operaciones numéricas y manejo de arrays\n",
        "import matplotlib.pyplot as plt # Para visualización de gráficos\n",
        "import seaborn as sns          # Para gráficos estadísticos y de exploración de datos\n",
        "from sklearn.model_selection import train_test_split # Para dividir datos en entrenamiento y prueba\n",
        "from sklearn.linear_model import LogisticRegression  # Modelo Logit básico\n",
        "from sklearn.linear_model import RidgeClassifier, Lasso, ElasticNet # Modelos logísticos penalizados\n",
        "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score # Para evaluar modelos\n",
        "from sklearn.preprocessing import StandardScaler    # Para normalizar datos\n",
        "from sklearn.model_selection import GridSearchCV    # Para optimizar hiperparámetros"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Análisis exploratorio"
      ],
      "metadata": {
        "id": "VsOEz8kfanSx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cargar el archivo de datos\n",
        "data_path = '/content/actividad-evaluativa3-AC/Dengue_Data.xlsx'\n",
        "data = pd.read_excel(data_path)\n",
        "\n",
        "# Visualizar las primeras filas\n",
        "data.head()"
      ],
      "metadata": {
        "id": "fUWw3j94LY-3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Información general del DataFrame\n",
        "print(\"Información general del dataset:\")\n",
        "data.info()\n"
      ],
      "metadata": {
        "id": "SYTCh3nFLuOS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Estadísticas descriptivas de las variables numéricas\n",
        "print(\"\\nEstadísticas descriptivas:\")\n",
        "data.describe()\n"
      ],
      "metadata": {
        "id": "vmON1HH8L-cZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualización de histogramas para variables numéricas\n",
        "data.hist(bins=20, figsize=(15, 10))\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "cLhHMpi7gI8l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualización de histogramas para variables categoricas\n",
        "\n",
        "# Selección de las columnas categóricas\n",
        "categorical_columns = data.select_dtypes(include=['object']).columns\n",
        "\n",
        "# Configuración de subplots en grupos de tres\n",
        "n_cols = 3\n",
        "n_rows = (len(categorical_columns) + n_cols - 1) // n_cols\n",
        "fig, axes = plt.subplots(nrows=n_rows, ncols=n_cols, figsize=(18, 6 * n_rows))\n",
        "\n",
        "# Aplanar los ejes en una sola lista para iterar fácilmente\n",
        "axes = axes.flatten()\n",
        "\n",
        "# Crear gráficos de barras para cada variable categórica\n",
        "for i, col in enumerate(categorical_columns):\n",
        "    sns.countplot(data=data, x=col, ax=axes[i])\n",
        "    axes[i].set_title(f'Frecuencia de {col}')\n",
        "    axes[i].tick_params(axis='x', rotation=45)\n",
        "    # Ajuste del límite superior de ylim para que se vea más espaciado\n",
        "    max_count = data[col].value_counts().max()\n",
        "    axes[i].set_ylim(0, max_count * 1.2)  # Ajuste del límite superior al 120% del valor máximo\n",
        "\n",
        "# Ocultar cualquier gráfico adicional en el subplot si hay un número impar de gráficos\n",
        "for j in range(i + 1, len(axes)):\n",
        "    axes[j].set_visible(False)\n",
        "\n",
        "# Ajuste del espaciado para evitar superposiciones\n",
        "plt.tight_layout(h_pad=5)  # h_pad controla el espacio vertical entre gráficos\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "rsFmPSUhr_f1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Identificación de valores nulos.\n",
        "Seleccionamos las columnas en las cuales los valores nulos representen mas del 50% del total de los datos"
      ],
      "metadata": {
        "id": "Bz12tf10Wkcv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Identificación de las columnas con datos Nulos\n",
        "\n",
        "# Calcular el umbral del 50% en función del número de filas\n",
        "threshold = len(data) * 0.5\n",
        "\n",
        "# Contar los valores nulos en cada columna y filtrar aquellas que superen el umbral del 50%\n",
        "null_counts = data.isnull().sum()\n",
        "high_null_columns = null_counts[null_counts > threshold].index.tolist()\n",
        "\n",
        "# Mostrar las columnas que tienen más del 50% de valores nulos\n",
        "print(\"Columnas con más del 50% de valores nulos:\", high_null_columns)\n",
        "\n",
        "# Gráfico de barras solo para columnas con valores nulos\n",
        "plt.figure(figsize=(30, 6))\n",
        "sns.barplot(x=null_counts.index, y=null_counts.values, palette=\"viridis\")\n",
        "plt.xticks(rotation=45)\n",
        "plt.xlabel(\"Variables con valores nulos\")\n",
        "plt.ylabel(\"Frecuencia de valores nulos\")\n",
        "plt.title(\"Frecuencia de valores nulos en variables seleccionadas\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Z0EKIYkcwvdn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Eliminamos las columnas en las cuales los datos nulos sean mayores al 50% de sus datos.\n",
        "data_filtered = data.drop(columns=high_null_columns)\n",
        "data_filtered\n"
      ],
      "metadata": {
        "id": "0VBUFELk8fra"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Eliminamos columnas repetidas"
      ],
      "metadata": {
        "id": "Pl4jOcG0XE-u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Eliminamos las columnas repetidas en este caso las de \"área_\"\n",
        "\n",
        "# Eliminar las columnas 'area_' y 'area_.1'\n",
        "data_filtered = data_filtered.drop(columns=['area_', 'area_.1'], errors='ignore')\n",
        "\n",
        "# Renombrar la columna 'area_.2' a 'area_'\n",
        "data_filtered = data_filtered.rename(columns={'area_.2': 'area_'})\n",
        "\n",
        "data_filtered\n"
      ],
      "metadata": {
        "id": "1Rj1gWZEBTIf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Identificar columnas que contienen \"No\" o \"Sí\"\n",
        "filtered_columns = [col for col in data_filtered.columns if data_filtered[col].astype(str).str.contains(\"No|Sí\", case=False, na=False).any()]\n",
        "\n",
        "# Cambiar \"Sí\" a 1 y \"No\" o nulos a 0 en las columnas identificadas\n",
        "for col in filtered_columns:\n",
        "    data_filtered[col] = data_filtered[col].apply(lambda x: 1 if str(x).strip().lower() == 'sí' else 0)\n",
        "\n",
        "# Mostrar las primeras filas para verificar el resultado\n",
        "data_filtered[filtered_columns].head()\n",
        "data_filtered"
      ],
      "metadata": {
        "id": "fHvjFSk7GBQY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Convertimos las demas variables categoricas en numericas (Grupos edad, sexo_, area_, y nom_eve)\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Identificamos columnas categóricas que necesitan codificación\n",
        "categorical_columns = data_filtered.select_dtypes(include=['object']).columns\n",
        "\n",
        "# Aplicamos codificación de etiquetas a cada columna categórica\n",
        "label_encoders = {}\n",
        "for col in categorical_columns:\n",
        "    le = LabelEncoder()\n",
        "    data_filtered[col] = le.fit_transform(data_filtered[col])\n",
        "    label_encoders[col] = le  # Guardamos el codificador para hacer referencia al mapeo más adelante si es necesario\n",
        "\n",
        "# Muestra el marco de datos transformado y las asignaciones para cada columna codificada.\n",
        "data_filtered.head(), {col: list(le.classes_) for col, le in label_encoders.items()}"
      ],
      "metadata": {
        "id": "AlURNpEmwURr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Valores asignados a cada categoria:\n",
        "\n",
        "Grupos edad:\n",
        "\n",
        "*   Adolescencia: 0\n",
        "*   Adultez: 1\n",
        "*   Infancia: 2\n",
        "*   Juventud: 3\n",
        "*   Primera infancia: 4\n",
        "*   Vejez: 5\n",
        "\n",
        "\n",
        "sexo_:\n",
        "\n",
        "* F (Femenino): 0\n",
        "* M (Masculino): 1\n",
        "\n",
        "\n",
        "area_:\n",
        "\n",
        "* Cabecera Municipal: 0\n",
        "* Centro Poblado: 1\n",
        "* Rural Disperso: 2\n",
        "\n",
        "nom_eve:\n",
        "\n",
        "* DENGUE: 0\n",
        "* DENGUE GRAVE: 1"
      ],
      "metadata": {
        "id": "3wCdB1A5xzcD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_filtered.head()"
      ],
      "metadata": {
        "id": "TjD8AuMUzO5a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_filtered.info()"
      ],
      "metadata": {
        "id": "JN5eW0XLHywo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "NpuIgBCpR4V-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Identificación de variables \"Sociodemograficas\" y \"Clinicas\""
      ],
      "metadata": {
        "id": "uB8r-tIoR4Xy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Identificar variables socio-demográficas y clínicas\n",
        "\n",
        "# Listado de las columnas\n",
        "columnas = data_filtered.columns\n",
        "\n",
        "# Variables socio-demográficas (basadas en nombres típicos y el contenido inspeccionado)\n",
        "variables_socio_demograficas = [\n",
        "    'edad_', 'Grupos edad', 'sexo_', 'area_', 'ocupacion_', 'per_etn_',\n",
        "    'gp_discapa', 'gp_desplaz', 'gp_migrant', 'gp_carcela', 'gp_indigen',\n",
        "    'gp_pobicbf', 'gp_desmovi', 'gp_vic_vio', 'gp_otros', 'Region'\n",
        "]\n",
        "\n",
        "# Variables clínicas/médicas (basadas en nombres de síntomas y el contenido inspeccionado)\n",
        "variables_clinicas = [\n",
        "    'fiebre', 'cefalea', 'dolrretroo', 'malgias', 'artralgia', 'erupcionr',\n",
        "    'dolor_abdo', 'vomito', 'diarrea', 'somnolenci', 'hipotensio',\n",
        "    'hepatomeg', 'hem_mucosa', 'hipotermia', 'caida_plaq', 'acum_liqui',\n",
        "    'aum_hemato'\n",
        "]\n",
        "\n",
        "# Variable objetivo para el primer análisis (Dengue o Dengue Grave)\n",
        "variable_objetivo_dengue = 'nom_eve'\n",
        "\n",
        "# Variable objetivo para el segundo análisis (Hospitalización)\n",
        "variable_objetivo_hospitalizacion = 'pac_hos_'\n",
        "\n",
        "variables_socio_demograficas, variables_clinicas\n"
      ],
      "metadata": {
        "id": "w28ESW9cSEoN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "\n",
        "variable_objetivo_dengue = 'nom_eve'\n",
        "\n",
        "# Filtrar los datos para las variables relevantes\n",
        "X = data_filtered[variables_socio_demograficas]\n",
        "y = data_filtered[variable_objetivo_dengue]\n",
        "\n",
        "# Separar los datos en conjunto de entrenamiento y prueba\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Escalar los datos\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "\n",
        "# Definir modelos con regularización\n",
        "models = {\n",
        "    'Logit': LogisticRegression(penalty=None, random_state=42), # Changed 'none' to None\n",
        "    'Logit Ridge': LogisticRegression(penalty='l2', C=1.0, random_state=42),\n",
        "    'Logit LASSO': LogisticRegression(penalty='l1', solver='saga', C=1.0, random_state=42),\n",
        "    'Logit Elastic Net': LogisticRegression(penalty='elasticnet', solver='saga', l1_ratio=0.5, C=1.0, random_state=42)\n",
        "}\n",
        "\n",
        "# Ajustar cada modelo y obtener los resultados\n",
        "model_results = {}\n",
        "for model_name, model in models.items():\n",
        "    model.fit(X_train_scaled, y_train)\n",
        "    y_pred = model.predict(X_test_scaled)\n",
        "    report = classification_report(y_test, y_pred, output_dict=True)\n",
        "    model_results[model_name] = {\n",
        "        'model': model,\n",
        "        'classification_report': report,\n",
        "        'coefficients': model.coef_\n",
        "    }\n",
        "\n",
        "model_results\n",
        "\n",
        "\n",
        "# Crear un DataFrame para almacenar los resultados de cada modelo\n",
        "results_df = pd.DataFrame(columns=['Model', 'Accuracy', 'Precision_0', 'Recall_0', 'F1_0', 'Precision_1', 'Recall_1', 'F1_1'])\n",
        "\n",
        "# Extraer métricas clave del reporte de clasificación de cada modelo\n",
        "for model_name, results in model_results.items():\n",
        "    report = results['classification_report']\n",
        "    # Obtener las métricas de precisión, recall y F1 para cada clase (0 y 1)\n",
        "    accuracy = report['accuracy']\n",
        "    precision_0 = report['0']['precision']\n",
        "    recall_0 = report['0']['recall']\n",
        "    f1_0 = report['0']['f1-score']\n",
        "    precision_1 = report['1']['precision']\n",
        "    recall_1 = report['1']['recall']\n",
        "    f1_1 = report['1']['f1-score']\n",
        "\n",
        "    # Agregar los resultados al DataFrame using pd.concat\n",
        "    results_df = pd.concat([results_df, pd.DataFrame([{\n",
        "        'Model': model_name,\n",
        "        'Accuracy': accuracy,\n",
        "        'Precision_0': precision_0,\n",
        "        'Recall_0': recall_0,\n",
        "        'F1_0': f1_0,\n",
        "        'Precision_1': precision_1,\n",
        "        'Recall_1': recall_1,\n",
        "        'F1_1': f1_1\n",
        "    }])], ignore_index=True)\n",
        "\n",
        "results_df"
      ],
      "metadata": {
        "id": "Ft4lfIHMGkVm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Conclusiones y Análisis:\n",
        "* Alta precisión general (0.99418): La precisión en todos los modelos es alta debido a la correcta clasificación de la clase mayoritaria (0 - \"DENGUE\"), mientras que ningún modelo logró clasificar correctamente la clase minoritaria (1 - \"DENGUE GRAVE\").\n",
        "\n",
        "* Desequilibrio de clases: La clase 1 no fue predicha correctamente por ningún modelo, reflejando el desbalance en los datos y la dificultad de los modelos para captar patrones en la clase minoritaria sin ajustes adicionales.\n",
        "\n",
        "* Regularización: Aunque se aplicaron penalizaciones (Ridge, LASSO y Elastic Net) para mejorar la estabilidad y selección de variables, no lograron mejorar la detección de la clase minoritaria."
      ],
      "metadata": {
        "id": "mpfhWvmFM7OO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Definir un rango de valores de regularización para analizar la evolución de coeficientes\n",
        "alphas = np.logspace(-4, 1, 100)\n",
        "\n",
        "# Inicializar estructuras para almacenar coeficientes en función de la penalización para cada modelo\n",
        "coef_ridge, coef_lasso, coef_enet = [], [], []\n",
        "\n",
        "# Calcular y almacenar coeficientes para Ridge, LASSO y Elastic Net\n",
        "for alpha in alphas:\n",
        "    # Ridge\n",
        "    ridge_model = LogisticRegression(penalty='l2', C=1/alpha, random_state=42, max_iter=10000)\n",
        "    ridge_model.fit(X_train_scaled, y_train)\n",
        "    coef_ridge.append(ridge_model.coef_.flatten())\n",
        "\n",
        "    # LASSO\n",
        "    lasso_model = LogisticRegression(penalty='l1', solver='saga', C=1/alpha, random_state=42, max_iter=10000)\n",
        "    lasso_model.fit(X_train_scaled, y_train)\n",
        "    coef_lasso.append(lasso_model.coef_.flatten())\n",
        "\n",
        "    # Elastic Net\n",
        "    enet_model = LogisticRegression(penalty='elasticnet', solver='saga', l1_ratio=0.5, C=1/alpha, random_state=42, max_iter=10000)\n",
        "    enet_model.fit(X_train_scaled, y_train)\n",
        "    coef_enet.append(enet_model.coef_.flatten())\n",
        "\n",
        "# Convertir listas de coeficientes en arrays para facilitar el trazado\n",
        "coef_ridge = np.array(coef_ridge)\n",
        "coef_lasso = np.array(coef_lasso)\n",
        "coef_enet = np.array(coef_enet)\n",
        "\n",
        "# Nombres de las variables para etiquetas\n",
        "variable_names = X.columns\n",
        "\n",
        "# Graficar evolución de coeficientes para cada modelo\n",
        "fig, axes = plt.subplots(1, 3, figsize=(18, 6), sharey=True)\n",
        "fig.suptitle(\"Evolución de Coeficientes en Función de la Penalización (α)\")\n",
        "\n",
        "# Gráfico para Ridge\n",
        "axes[0].set_title(\"Ridge\")\n",
        "for i, coef in enumerate(coef_ridge.T):\n",
        "    axes[0].plot(alphas, coef, label=variable_names[i])\n",
        "axes[0].set_xscale(\"log\")\n",
        "axes[0].invert_xaxis()\n",
        "axes[0].set_xlabel(\"Penalización (α)\")\n",
        "axes[0].set_ylabel(\"Coeficiente\")\n",
        "\n",
        "# Gráfico para LASSO\n",
        "axes[1].set_title(\"LASSO\")\n",
        "for i, coef in enumerate(coef_lasso.T):\n",
        "    axes[1].plot(alphas, coef, label=variable_names[i])\n",
        "axes[1].set_xscale(\"log\")\n",
        "axes[1].invert_xaxis()\n",
        "axes[1].set_xlabel(\"Penalización (α)\")\n",
        "\n",
        "# Gráfico para Elastic Net\n",
        "axes[2].set_title(\"Elastic Net\")\n",
        "for i, coef in enumerate(coef_enet.T):\n",
        "    axes[2].plot(alphas, coef, label=variable_names[i])\n",
        "axes[2].set_xscale(\"log\")\n",
        "axes[2].invert_xaxis()\n",
        "axes[2].set_xlabel(\"Penalización (α)\")\n",
        "\n",
        "# Agregar leyendas\n",
        "axes[0].legend(loc='upper right', bbox_to_anchor=(1.5, 1), ncol=1)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ac7kpvASOb_g"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}